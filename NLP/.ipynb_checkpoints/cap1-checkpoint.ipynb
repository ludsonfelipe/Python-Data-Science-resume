{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = [('eu sou admirada por muitos','alegria'),\n",
    "        ('me sinto completamente amado','alegria'),\n",
    "        ('amar e maravilhoso','alegria'),\n",
    "        ('estou me sentindo muito animado novamente','alegria'),\n",
    "        ('eu estou muito bem hoje','alegria'),\n",
    "        ('que belo dia para dirigir um carro novo','alegria'),\n",
    "        ('o dia está muito bonito','alegria'),\n",
    "        ('estou contente com o resultado do teste que fiz no dia de ontem','alegria'),\n",
    "        ('o amor e lindo','alegria'),\n",
    "        ('nossa amizade e amor vai durar para sempre', 'alegria'),\n",
    "        ('estou amedrontado', 'medo'),\n",
    "        ('ele esta me ameacando a dias', 'medo'),\n",
    "        ('isso me deixa apavorada', 'medo'),\n",
    "        ('este lugar e apavorante', 'medo'),\n",
    "        ('se perdermos outro jogo seremos eliminados e isso me deixa com pavor', 'medo'),\n",
    "        ('tome cuidado com o lobisomem', 'medo'),\n",
    "        ('se eles descobrirem estamos encrencados', 'medo'),\n",
    "        ('estou tremendo de medo', 'medo'),\n",
    "        ('eu tenho muito medo dele', 'medo'),\n",
    "        ('estou com medo do resultado dos meus testes', 'medo')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('portuguese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_finished = []\n",
    "for frase, emot in base:\n",
    "    lista_palavras = frase.split(' ')\n",
    "    lista_clean = []\n",
    "    for palavra in lista_palavras:\n",
    "        if palavra not in stop_words:\n",
    "            lista_clean.append(palavra)\n",
    "    base_finished.append((lista_clean,emot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['admirada', 'muitos'], 'alegria'),\n",
       " (['sinto', 'completamente', 'amado'], 'alegria'),\n",
       " (['amar', 'maravilhoso'], 'alegria'),\n",
       " (['sentindo', 'animado', 'novamente'], 'alegria'),\n",
       " (['bem', 'hoje'], 'alegria'),\n",
       " (['belo', 'dia', 'dirigir', 'carro', 'novo'], 'alegria'),\n",
       " (['dia', 'bonito'], 'alegria'),\n",
       " (['contente', 'resultado', 'teste', 'fiz', 'dia', 'ontem'], 'alegria'),\n",
       " (['amor', 'lindo'], 'alegria'),\n",
       " (['amizade', 'amor', 'vai', 'durar', 'sempre'], 'alegria'),\n",
       " (['amedrontado'], 'medo'),\n",
       " (['ameacando', 'dias'], 'medo'),\n",
       " (['deixa', 'apavorada'], 'medo'),\n",
       " (['lugar', 'apavorante'], 'medo'),\n",
       " (['perdermos', 'outro', 'jogo', 'eliminados', 'deixa', 'pavor'], 'medo'),\n",
       " (['tome', 'cuidado', 'lobisomem'], 'medo'),\n",
       " (['descobrirem', 'encrencados'], 'medo'),\n",
       " (['tremendo', 'medo'], 'medo'),\n",
       " (['medo'], 'medo'),\n",
       " (['medo', 'resultado', 'testes'], 'medo')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_finished = []\n",
    "for frase, emot in base:\n",
    "    lista_palavras = frase.split(' ')\n",
    "    lista_clean = []\n",
    "    for palavra in lista_palavras:\n",
    "        if palavra not in stop_words:\n",
    "            lista_clean.append(str(stem.stem(palavra)))\n",
    "    base_finished.append((lista_clean,emot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['admir', 'muit'], 'alegria'),\n",
       " (['sint', 'complet', 'am'], 'alegria'),\n",
       " (['am', 'maravilh'], 'alegria'),\n",
       " (['sent', 'anim', 'nov'], 'alegria'),\n",
       " (['bem', 'hoj'], 'alegria'),\n",
       " (['bel', 'dia', 'dirig', 'carr', 'nov'], 'alegria'),\n",
       " (['dia', 'bonit'], 'alegria'),\n",
       " (['cont', 'result', 'test', 'fiz', 'dia', 'ont'], 'alegria'),\n",
       " (['am', 'lind'], 'alegria'),\n",
       " (['amizad', 'am', 'vai', 'dur', 'sempr'], 'alegria'),\n",
       " (['amedront'], 'medo'),\n",
       " (['ameac', 'dia'], 'medo'),\n",
       " (['deix', 'apavor'], 'medo'),\n",
       " (['lug', 'apavor'], 'medo'),\n",
       " (['perd', 'outr', 'jog', 'elimin', 'deix', 'pav'], 'medo'),\n",
       " (['tom', 'cuid', 'lobisom'], 'medo'),\n",
       " (['descobr', 'encrenc'], 'medo'),\n",
       " (['trem', 'med'], 'medo'),\n",
       " (['med'], 'medo'),\n",
       " (['med', 'result', 'test'], 'medo')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separando palavras repetitivas com FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_juntas = []\n",
    "for frases, emocao in base_finished:\n",
    "    palavras_juntas.extend(frases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencia = nltk.FreqDist(palavras_juntas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'am': 4, 'dia': 4, 'med': 3, 'nov': 2, 'result': 2, 'test': 2, 'deix': 2, 'apavor': 2, 'admir': 1, 'muit': 1, ...})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['admir', 'muit', 'sint', 'complet', 'am', 'maravilh', 'sent', 'anim', 'nov', 'bem', 'hoj', 'bel', 'dia', 'dirig', 'carr', 'bonit', 'cont', 'result', 'test', 'fiz', 'ont', 'lind', 'amizad', 'vai', 'dur', 'sempr', 'amedront', 'ameac', 'deix', 'apavor', 'lug', 'perd', 'outr', 'jog', 'elimin', 'pav', 'tom', 'cuid', 'lobisom', 'descobr', 'encrenc', 'trem', 'med'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencia.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efetuando comparação das palavras com as palavras da lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrator(documento):\n",
    "    doc = set(documento)\n",
    "    caracteristicas = {}\n",
    "    for palavras in nltk.FreqDist(palavras_juntas).keys():\n",
    "        caracteristicas[palavras] = ( palavras in doc)\n",
    "    return caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_completa = nltk.classify.apply_features(extrator,base_finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'admir': True, 'muit': True, 'sint': False, 'complet': False, 'am': False, 'maravilh': False, 'sent': False, 'anim': False, 'nov': False, 'bem': False, 'hoj': False, 'bel': False, 'dia': False, 'dirig': False, 'carr': False, 'bonit': False, 'cont': False, 'result': False, 'test': False, 'fiz': False, 'ont': False, 'lind': False, 'amizad': False, 'vai': False, 'dur': False, 'sempr': False, 'amedront': False, 'ameac': False, 'deix': False, 'apavor': False, 'lug': False, 'perd': False, 'outr': False, 'jog': False, 'elimin': False, 'pav': False, 'tom': False, 'cuid': False, 'lobisom': False, 'descobr': False, 'encrenc': False, 'trem': False, 'med': False}, 'alegria'), ({'admir': False, 'muit': False, 'sint': True, 'complet': True, 'am': True, 'maravilh': False, 'sent': False, 'anim': False, 'nov': False, 'bem': False, 'hoj': False, 'bel': False, 'dia': False, 'dirig': False, 'carr': False, 'bonit': False, 'cont': False, 'result': False, 'test': False, 'fiz': False, 'ont': False, 'lind': False, 'amizad': False, 'vai': False, 'dur': False, 'sempr': False, 'amedront': False, 'ameac': False, 'deix': False, 'apavor': False, 'lug': False, 'perd': False, 'outr': False, 'jog': False, 'elimin': False, 'pav': False, 'tom': False, 'cuid': False, 'lobisom': False, 'descobr': False, 'encrenc': False, 'trem': False, 'med': False}, 'alegria'), ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_completa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um classificador NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(base_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alegria', 'medo']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Targets\n",
    "classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     dia = True           alegri : medo   =      2.3 : 1.0\n",
      "                      am = False            medo : alegri =      1.6 : 1.0\n",
      "                     med = False          alegri : medo   =      1.4 : 1.0\n",
      "                     dia = False            medo : alegri =      1.3 : 1.0\n",
      "                  apavor = False          alegri : medo   =      1.2 : 1.0\n",
      "                    deix = False          alegri : medo   =      1.2 : 1.0\n",
      "                     nov = False            medo : alegri =      1.2 : 1.0\n",
      "                   admir = False            medo : alegri =      1.1 : 1.0\n",
      "                   ameac = False          alegri : medo   =      1.1 : 1.0\n",
      "                amedront = False          alegri : medo   =      1.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Probabilidade de palavras relacionadas a emoções\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza de dados para predict e predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"isso é apavorante\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_test = []\n",
    "for i in text.split():\n",
    "    stemming_test.append(stem.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iss', 'é', 'apavor']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medo\n"
     ]
    }
   ],
   "source": [
    "print(classifier.classify(extrator(stemming_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = classifier.prob_classify(extrator(stemming_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alegria 0.0635950881505187\n",
      "medo 0.9364049118494808\n"
     ]
    }
   ],
   "source": [
    "for classe in values.samples():\n",
    "    print(classe, values.prob(classe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk.classify.accuracy(classifier,base_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from nltk.metrics import ConfusionMatrix \n",
    "# matrix = ConfusionMatrix(lista_esperado,lista_previsto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fb9f276e0f50a8c59c9b1bd8fbe1fb75b2bae3f0a31df33a26f64d22b7ba6169"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
